{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "from UModel import UNet\n",
    "import Config\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from torchmetrics.functional import dice\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import cv2\n",
    "from focal_loss import FocalLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from class_weights import compute_class_weight\n",
    "import Early_stopping as E\n",
    "import torch.nn as nn\n",
    "\n",
    "## load the image and mask filepaths in a sorted manner ##\n",
    "\n",
    "imagePaths = np.array(sorted(list(paths.list_images(Config.Image_dataset_dir))))\n",
    "maskPaths = np.array(sorted(list(paths.list_images(Config.Mask_dataset_dir))))\n",
    "\n",
    "# Taking out the test set and leaving the rest of the data for cross validation\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(imagePaths, maskPaths, train_size=0.9, random_state=42, shuffle = True)\n",
    "\n",
    "print(\"Saving testing image paths...\")\n",
    "f = open(Config.TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(X_test))\n",
    "f.close()\n",
    "\n",
    "# Setting up the Kfold cross validation with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, val_index in kf.split(X_rest, y_rest):\n",
    "    # Splitting the data into training and validation sets\n",
    "    X_train, X_val = X_rest[train_index], X_rest[val_index]\n",
    "    y_train, y_val = y_rest[train_index], y_rest[val_index]\n",
    "    \n",
    "    # Define transformations\n",
    "    train_transform = A.Compose([A.Resize(Config.Input_Height, Config.Input_Width),\n",
    "                                 A.Normalize(mean=(0.0), std=(1.0)),\n",
    "                                 A.HorizontalFlip(p=0.5), \n",
    "                                 A.RandomRotate90(p=0.5),\n",
    "                                 ToTensorV2()])\n",
    "    \n",
    "    val_transform = A.Compose([A.Resize(Config.Input_Height, Config.Input_Width),\n",
    "                               A.Normalize(mean=(0.0), std=(1.0)),\n",
    "                               A.VerticalFlip(p=0.5), \n",
    "                               A.RandomRotate90(p=0.5),\n",
    "                               ToTensorV2()])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create the train and validation datasets\n",
    "    trainDS = dataloader.MyDataset(imagePaths=X_train, maskPaths=y_train, transform=train_transform)\n",
    "    valDS = dataloader.MyDataset(imagePaths=X_val, maskPaths=y_val, transform=val_transform)\n",
    "    print(f\"There are {len(trainDS)} samples in the training set\")\n",
    "    print(f\"There are {len(valDS)} samples in the validation set\")\n",
    "    print('************************************************')\n",
    "\n",
    "    # Create the train and validation dataloaders\n",
    "    trainLoader = DataLoader(trainDS, shuffle=True, batch_size=Config.Batch_size,\n",
    "                             pin_memory=Config.PIN_MEMORY)\n",
    "\n",
    "    valLoader = DataLoader(valDS, shuffle=True, batch_size=Config.Batch_size,\n",
    "                           pin_memory=Config.PIN_MEMORY)\n",
    "    \n",
    "    class_weight = compute_class_weight(trainLoader, Config.No_classes).to(Config.DEVICE)\n",
    "    print(class_weight)\n",
    "    \n",
    "    # initialize our UNet model\n",
    "    unet = UNet(n_channels=Config.No_channels, n_classes=Config.No_classes, bilinear=False).to(Config.DEVICE)\n",
    "\n",
    "    # initialize Binary cross entropy with logit loss function and Adam optimizer\n",
    "    lossFunc = FocalLoss(weight = class_weight)\n",
    "    opt = Adam(unet.parameters(), lr=Config.Init_LR)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(opt, 'max', patience=2)\n",
    "    early_stopper = E.EarlyStopper(patience=2, min_delta=0.01)\n",
    "\n",
    "    # calculate steps per epoch for training, validation and test set\n",
    "    trainSteps = len(trainDS) // Config.Batch_size\n",
    "    valSteps = len(valDS) // Config.Batch_size\n",
    "\n",
    "    # initialize a dictionary to store training history\n",
    "    CE_loss = {\"train_loss\": [], \"val_loss\": []}\n",
    "    dsc_loss = {\"Dice_train\": [], \"Dice_val\": []}\n",
    "\n",
    "    # loop over epochs\n",
    "    print(\"Training the network...\")\n",
    "    startTime = time.time()\n",
    "\n",
    "    for e in tqdm(range(Config.Num_epochs)):\n",
    "        # set the model in training mode\n",
    "        unet.train()\n",
    "\n",
    "        # initialize the total training and validation loss0\n",
    "        totalTrainLoss, totalValLoss, dice_score_train, dice_score_val = 0, 0, 0, 0\n",
    "\n",
    "        # loop over the training set\n",
    "        for (i, (x, y)) in enumerate(trainLoader):\n",
    "            # send the input to the device\n",
    "            (x, y) = (x.to(Config.DEVICE), y.to(Config.DEVICE))\n",
    "\n",
    "            # perform a forward pss and calculate the training loss\n",
    "            pred = unet(x)\n",
    "            CEloss = lossFunc(pred, y)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            dice_loss = dice(softmax(pred), y)\n",
    "            total_loss = (1 - dice_loss) + CEloss\n",
    "\n",
    "            # first, zero out any previously accumulated gradients, then\n",
    "            # perform backpropagation, and then update model parameters\n",
    "            opt.zero_grad()\n",
    "            total_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # add the loss to the total training loss so far\n",
    "            totalTrainLoss += CEloss\n",
    "            dice_score_train += dice_loss\n",
    "\n",
    "        # switch off autograd\n",
    "        with torch.no_grad():\n",
    "            # set the model in evaluation mode\n",
    "            unet.eval()\n",
    "\n",
    "            # loop over the validation set\n",
    "            for (x, y) in valLoader:\n",
    "                # send the input to the device\n",
    "                (x, y) = (x.to(Config.DEVICE), y.to(Config.DEVICE))\n",
    "\n",
    "                # make the predictions and calculate the validation loss\n",
    "                pred = unet(x)\n",
    "                totalValLoss += lossFunc(pred, y)\n",
    "                softmax = nn.Softmax(dim=1)\n",
    "                dice_score_val += dice(softmax(pred), y)\n",
    "\n",
    "        # calculate the average training and validation loss\n",
    "        avgTrainLoss = totalTrainLoss / trainSteps\n",
    "        avgValLoss = totalValLoss / valSteps\n",
    "        avgDiceTrain = dice_score_train / trainSteps\n",
    "        avgDiceVal = dice_score_val / valSteps\n",
    "        \n",
    "        # Checking the validation loss if not changing for early stopping\n",
    "#         if early_stopper.early_stop(avgValLoss):\n",
    "#             break\n",
    "        \n",
    "        # Adapting the learining rate based on maximized Dice Score\n",
    "        scheduler.step(avgValLoss)\n",
    "        print(opt.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        # update the training history\n",
    "        CE_loss[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "        CE_loss[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "        dsc_loss[\"Dice_train\"].append(avgDiceTrain.cpu().detach().numpy())\n",
    "        dsc_loss[\"Dice_val\"].append(avgDiceVal.cpu().detach().numpy())\n",
    "\n",
    "        # print the model training and validation information\n",
    "        print(\"EPOCH: {}/{}\".format(e + 1, Config.Num_epochs))\n",
    "        print(\"Train loss: {:.4f}, Validation loss: {:.4f}\".format(avgTrainLoss, avgValLoss))\n",
    "        print(\"Training Dice Score: {:.4f}% , Validation Dice Score: {:.4f}%\".format(avgDiceTrain * 100, avgDiceVal * 100))\n",
    "        del dice_score_train, dice_score_val, avgDiceTrain, avgDiceVal, totalTrainLoss, totalValLoss, avgTrainLoss, avgValLoss\n",
    "\n",
    "    # display the total time needed to perform the training\n",
    "    endTime = time.time()\n",
    "    print(\"Total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
    "    \n",
    "    # plot the training loss\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(2, 1, 1)\n",
    "    plt.plot(CE_loss[\"train_loss\"], label=\"train_loss\")\n",
    "    plt.plot(CE_loss[\"val_loss\"], label=\"validation_loss\")\n",
    "    plt.title(\"Training Loss vs. Validation Loss\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(2, 1, 2)\n",
    "    plt.plot(dsc_loss[\"Dice_train\"], label=\"train_dice_score\")\n",
    "    plt.plot(dsc_loss[\"Dice_val\"], label=\"validation_dice_score\")\n",
    "    plt.title(\"Training Dice Score vs. Validation Dice Score\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Dice Score\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # serialize the model to disk\n",
    "    torch.save(unet, Config.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26137d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "for img, mask in trainLoader:\n",
    "#     img = np.array(img)\n",
    "#     plt.hist(img.ravel() , bins = 50, density = True)\n",
    "    print('The shape of the image is: {}'.format(img.shape))\n",
    "    print('The type of the image is: {}'.format(img.dtype))\n",
    "    print('The shape of the mask is: {}'.format(mask.shape))\n",
    "    print('The shape of the mask is: {}'.format(mask.dtype))\n",
    "    print(torch.unique(mask) , torch.unique(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for img,mask in trainLoader:\n",
    "    figure, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 18))\n",
    "    img = np.transpose(img[0,:,:,:],(1,2,0))\n",
    "    mask = np.array(mask[0,:,:])\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(mask)\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].set_title(\"Ground Truth\")\n",
    "    ax[0].grid(False)\n",
    "    ax[1].grid(False)\n",
    "    figure.tight_layout()\n",
    "    figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model= torch.load('C:\\\\Users\\\\Jessica NT MCA\\\\Desktop\\\\MA_Abdelrahman\\\\Master Thesis Project\\\\Multi-class UNET\\\\Dataset\\\\output\\\\unet_out.pth').to(Config.DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(Config.DEVICE)\n",
    "            y = y.to(Config.DEVICE)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            preds = softmax(model(x.float()))\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
    "\n",
    "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25318799",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(trainLoader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(valLoader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79419261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from UModel import UNet\n",
    "import torch\n",
    "import Config\n",
    "import dataloader\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "filenames = []\n",
    "\n",
    "path = 'C:\\\\Users\\\\z004b1tz\\\\Desktop\\\\Master Thesis Project\\\\Multi-class UNET\\\\Dataset\\\\output\\\\test_paths.txt'\n",
    "\n",
    "lines = open(path,\"r\").read().split(\"\\n\")\n",
    "for line in lines:\n",
    "    filename = os.path.basename(line)\n",
    "    filenames.append(filename)\n",
    "    X_test.append(line)\n",
    "    y = 'C:\\\\Users\\\\z004b1tz\\\\Desktop\\\\Master Thesis Project\\\\Multi-class UNET\\\\Dataset\\\\Masks\\\\'+filename\n",
    "    y_test.append(y)\n",
    "\n",
    "test_transform = A.Compose([A.Resize(Config.Input_Height, Config.Input_Width),\n",
    "                               A.Normalize(mean=(0.0), std=(1.0)),\n",
    "                               ToTensorV2()])\n",
    "testDS = dataloader.MyDataset(imagePaths=X_test, maskPaths=y_test, transform=test_transform)\n",
    "testLoader = DataLoader(testDS, shuffle=False, batch_size=Config.Batch_size,\n",
    "                             pin_memory=Config.PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "model= torch.load('C:\\\\Users\\\\z004b1tz\\\\Desktop\\\\Master Thesis Project\\\\Multi-class UNET\\Dataset\\\\output\\\\unet_out.pth', map_location='cpu')\n",
    "model.eval()\n",
    "\n",
    "image_no = 0\n",
    "for x,y in testLoader:\n",
    "    x = x.to(Config.DEVICE)\n",
    "    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    preds = torch.argmax(softmax(model(x.float())), axis = 1).cpu().detach().numpy()\n",
    "    img = np.transpose(np.array(x[0,:,:,:].to('cpu')),(1,2,0))\n",
    "    preds = np.array(preds[0,:,:])\n",
    "    mask = np.array(y[0,:,:])\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(mask)\n",
    "    ax[2].imshow(preds)\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].set_title(\"Ground Truth\")\n",
    "    ax[2].set_title(\"Prediction\")\n",
    "    ax[0].grid(False)\n",
    "    ax[1].grid(False)\n",
    "    ax[2].grid(False)\n",
    "    cv2.imwrite('C:\\\\Users\\\\z004b1tz\\\\Desktop\\\\Master Thesis Project\\\\Multi-class UNET\\\\Dataset\\\\output\\\\predictions\\\\'+filenames[image_no], preds)\n",
    "    figure.tight_layout()\n",
    "    figure\n",
    "    image_no +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cbeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop ##\n",
    "model = UNet(1,3).to(Config.DEVICE)\n",
    "loss_fn = FocalLoss(weight = torch.FloatTensor([0.02,0.38,0.9]).to(Config.DEVICE))\n",
    "opt = Adam(model.parameters() , lr = Config.Init_LR)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=30)\n",
    "loop = tqdm(range(10))\n",
    "train_loss = []\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "# loop over the training set\n",
    "\n",
    "for e in loop:\n",
    "    for (i, (x, y)) in enumerate(trainLoader):\n",
    "        model.train()\n",
    "        # send the input to the device\n",
    "        x = x.to(Config.DEVICE)\n",
    "        y = y.to(Config.DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x.float())\n",
    "    #         print(pred.shape)\n",
    "            CEloss = loss_fn(pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        CEloss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Adapting the learining rate based on maximized Dice Score\n",
    "        scheduler.step(CEloss)\n",
    "#         print(opt.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        loop.set_postfix(loss=CEloss.item())\n",
    "        train_loss.append(CEloss.cpu().detach().numpy())\n",
    "        del CEloss\n",
    "\n",
    "torch.save(model.state_dict(), Config.MODEL_PATH)\n",
    "endTime = time.time()\n",
    "print(\"Total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a82a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
